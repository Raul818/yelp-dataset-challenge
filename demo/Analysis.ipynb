{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from datetime import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json as json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write_df_to_json_file(df, filename):\n",
    "    df.reset_index().to_json(filename,orient='records')\n",
    "\n",
    "def write_preprocessed_data(df_business_restaurants):\n",
    "    write_df_to_json_file(df_business_restaurants,\"../out/preprocessed_business_data.json\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Helper methods for pre-processing\n",
    "\n",
    "WORKING_TYPES = {\n",
    "        \"WEEKEND_TYPE\": \"weekend\",\n",
    "        \"BREAKFAST_TYPE\": \"breakfast\",\n",
    "        \"LUNCH_TYPE\": \"lunch\",\n",
    "        \"AFTER_LUNCH_TYPE\": \"after-lunch\",\n",
    "        \"DINNER_TYPE\": \"dinner\",\n",
    "        \"NIGHT_TYPE\": \"night\",\n",
    "    }\n",
    "\n",
    "breakfast = time(8)\n",
    "lunch = time(12)\n",
    "after_lunch = time(15)\n",
    "dinner = time(18)\n",
    "night = time(0)\n",
    "\n",
    "def get_avg_score_of_business(series, bid, col):\n",
    "    if bid in series.groups:\n",
    "        group = series.get_group(bid)\n",
    "        return group[col].mean()\n",
    "    else:\n",
    "        return np.nan\n",
    "    \n",
    "def add_evluation_score_to_business(new_col_for_score_df, df_business_restaurants):\n",
    "    series = new_col_for_score_df[0].groupby('business_id')\n",
    "    df_business_restaurants[new_col_for_score_df[2]] = df_business_restaurants['business_id'].apply(lambda bid: get_avg_score_of_business(series, bid, new_col_for_score_df[1]))\n",
    "    return df_business_restaurants\n",
    "    \n",
    "def get_checkin_count(df_checkin, bid):\n",
    "    row = df_checkin[df_checkin['business_id'] == bid]\n",
    "    if not row.empty:\n",
    "        return row.iloc[0]['checkin_count']\n",
    "    else:\n",
    "        return np.nan\n",
    "    \n",
    "def score_to_rating(col, df_business_restaurants):\n",
    "     ## calculate the rating from scores\n",
    "    NUM_BINS_OF_RATING = 72\n",
    "    bins = np.linspace(\n",
    "        df_business_restaurants[col].min(),\n",
    "        df_business_restaurants[col].max(),\n",
    "        num=NUM_BINS_OF_RATING)\n",
    "    df_business_restaurants[col] = df_business_restaurants[col].apply(\n",
    "        # return the rating which is closeat to the score.\n",
    "        lambda score: np.argmin(np.abs(bins - score)) + 1)\n",
    "    return df_business_restaurants\n",
    "\n",
    "def in_between(start, end, check):\n",
    "    if start == end: # 24 hours\n",
    "        return True\n",
    "    if start < end:\n",
    "        return start <= check < end\n",
    "    else: # over midnight e.g., 23:30-04:15\n",
    "        return start <= check or check < end\n",
    "\n",
    "TYPE_THRESHOLD = 1\n",
    "def get_available_working_type(c, wt):\n",
    "    if c >= TYPE_THRESHOLD:\n",
    "        return [wt]\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "def spec_hours_to_type(s):\n",
    "    types = []\n",
    "\n",
    "    breakfast_count = 0\n",
    "    lunch_count = 0\n",
    "    after_lunch_count = 0\n",
    "    dinner_count = 0\n",
    "    night_count = 0\n",
    "\n",
    "    for day in s:\n",
    "\n",
    "        clo = s[day]['close']\n",
    "        op = s[day]['open']\n",
    "\n",
    "        h, m = clo.split(':')\n",
    "        clo_t = time(int(h), int(m))\n",
    "\n",
    "        h, m = op.split(':')\n",
    "        op_t = time(int(h), int(m))\n",
    "\n",
    "        breakfast_count += int(in_between(op_t, clo_t, breakfast))\n",
    "        lunch_count += int(in_between(op_t, clo_t, lunch))\n",
    "        after_lunch_count += int(in_between(op_t, clo_t, after_lunch))\n",
    "        dinner_count += int(in_between(op_t, clo_t, dinner))\n",
    "        night_count += int(in_between(op_t, clo_t, night))\n",
    "\n",
    "        if (day in ['Saturday', 'Sunday']) and (WORKING_TYPES[\"WEEKEND_TYPE\"] not in types):\n",
    "            types.append(WORKING_TYPES[\"WEEKEND_TYPE\"])\n",
    "\n",
    "    types += get_available_working_type(breakfast_count, WORKING_TYPES[\"BREAKFAST_TYPE\"])\n",
    "    types += get_available_working_type(lunch_count, WORKING_TYPES[\"LUNCH_TYPE\"])\n",
    "    types += get_available_working_type(after_lunch_count, WORKING_TYPES[\"AFTER_LUNCH_TYPE\"])\n",
    "    types += get_available_working_type(dinner_count, WORKING_TYPES[\"DINNER_TYPE\"])\n",
    "    types += get_available_working_type(night_count, WORKING_TYPES[\"NIGHT_TYPE\"])\n",
    "\n",
    "    return join_types(types)\n",
    "\n",
    "def hours_to_type(s):\n",
    "    if isinstance(s, str):\n",
    "        return s\n",
    "\n",
    "    if s:\n",
    "        return spec_hours_to_type(s)\n",
    "    else:\n",
    "        return join_types(WORKING_TYPES.values())\n",
    "\n",
    "def join_types(ts):\n",
    "    # reorder\n",
    "    ordered_types = []\n",
    "    for t in WORKING_TYPES.values():\n",
    "        if t in ts:\n",
    "            ordered_types.append(t)\n",
    "    return '_'.join(ordered_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_preprocessed_data():\n",
    "    \n",
    "    DATASET_DIR = '../'\n",
    "    \n",
    "    ## read reviews and calculate sentiment scores\n",
    "    df_review = pd.read_json('../out/yelp_academic_dataset_review_sentiment.json', lines=True)\n",
    "    df_review = df_review[df_review['sentiment_value'] != 3]    # remove reviews with invalid sentiment_value\n",
    "    df_review = df_review.assign(sentiment_score = \n",
    "                                 lambda df: df['sentiment_value'] * df['votes'].apply(lambda s: s['useful'] + 1))\n",
    "    ## read tips and calculate sentiment scores\n",
    "    df_tip = pd.read_json('../out/yelp_academic_dataset_tip_sentiment.json', lines=True)\n",
    "    df_tip = df_tip[df_tip['sentiment_value'] != 3]    # remove reviews with invalid sentiment_value\n",
    "    df_tip = df_tip.assign(sentiment_score =\n",
    "            lambda df: df['sentiment_value'] * (df['likes'] + 1))\n",
    "    \n",
    "    ## read business dataset\n",
    "    df_business = pd.read_json(DATASET_DIR + '/yelp_academic_dataset_business.json', lines=True)\n",
    "    business_filters = (df_business['review_count'].apply(lambda rc: rc >= 20)\n",
    "                    & df_business['categories'].apply(lambda cs: 'Restaurants' in cs)\n",
    "                    & df_business['open'])\n",
    "    df_business_restaurants = (df_business[business_filters]\n",
    "                           .reset_index(drop=True)[['business_id', 'stars', 'review_count', 'hours', 'city', 'attributes']])\n",
    "    # round the stars\n",
    "    df_business_restaurants['stars'] = df_business_restaurants['stars'].apply(np.round)\n",
    "    \n",
    "    new_col_for_score_dfs = [(df_review, 'sentiment_score', 'review_sentiment_rating'),\n",
    "                         (df_review, 'stars', 'review_star_rating'),\n",
    "                         (df_tip, 'sentiment_score', 'tip_rating')]\n",
    "    \n",
    "    for item in new_col_for_score_dfs:\n",
    "        df_business_restaurants = add_evluation_score_to_business(item, df_business_restaurants)\n",
    "\n",
    "\n",
    "    ## read checkin count of business and calculate \"checkin_rating\"\n",
    "    df_checkin = pd.read_json('../out/business_with_checkin_count.json')\n",
    "    \n",
    "    df_business_restaurants['checkin_rating'] = df_business_restaurants['business_id'].apply(lambda bid: get_checkin_count(df_checkin,bid))\n",
    "\n",
    "    df_business_restaurants['checkin_rating'] = df_business_restaurants['review_count'] / df_business_restaurants['checkin_rating']\n",
    "\n",
    "\n",
    "    ## filter out businesses with nan values\n",
    "    business_ratings_cols = ['review_sentiment_rating',\n",
    "    #                          'review_star_rating',\n",
    "                             'tip_rating',]\n",
    "    #                          'checkin_rating']\n",
    "\n",
    "    business_nan_filters = np.ones(len(df_business_restaurants), dtype=bool)\n",
    "    for col in business_ratings_cols:\n",
    "        business_nan_filters &= np.invert(np.isnan(df_business_restaurants[col]))\n",
    "\n",
    "    df_business_restaurants = df_business_restaurants[business_nan_filters].reset_index(drop=True)\n",
    "\n",
    "\n",
    "   \n",
    "    score_cols = ['review_sentiment_rating', 'review_star_rating', 'tip_rating', 'checkin_rating']\n",
    "\n",
    "    # convert score to rating\n",
    "    for col in score_cols:\n",
    "        df_business_restaurants = score_to_rating(col, df_business_restaurants)\n",
    "\n",
    "\n",
    "    ## convert \"hours\" in business to \"working_type\"\n",
    "    df_business_restaurants = df_business_restaurants.assign(working_type=lambda x: x['hours'])\n",
    "\n",
    "    \n",
    "    \n",
    "    df_business_restaurants['working_type'] = df_business_restaurants['working_type'].apply(hours_to_type)\n",
    "    working_type_set = df_business_restaurants['working_type'].unique()\n",
    "\n",
    "    return df_business_restaurants\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_business_restaurants = get_preprocessed_data()\n",
    "#df_business_restaurants.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['business_id', 'stars', 'review_count', 'hours', 'city', 'attributes',\n",
       "       'review_sentiment_rating', 'review_star_rating', 'tip_rating',\n",
       "       'checkin_rating', 'working_type'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_business_restaurants.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#write_preprocessed_data(df_business_restaurants)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_business_bystars(training_restaurants_df, stars_column):\n",
    "    group_by_stars = training_restaurants_df.groupby(stars_column)\n",
    "    business_of_stars = {}\n",
    "    for star in group_by_stars.groups:\n",
    "        group = group_by_stars.get_group(star)\n",
    "        business_of_stars[star] = group.assign(working_type=lambda x: x['hours'])\n",
    "    return business_of_stars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import normalize\n",
    "def get_priors(business_of_stars,training_restaurants_df):\n",
    "    prior_of_stars = {}\n",
    "    for star in business_of_stars:\n",
    "        prior_of_stars[star] = len(business_of_stars[star]) * 1.0 / len(training_restaurants_df)\n",
    "    #print (prior_of_stars)\n",
    "    x = list(prior_of_stars.values())\n",
    "    normalizing_fact = 1 / np.linalg.norm(x)\n",
    "    for k in prior_of_stars:\n",
    "        prior_of_stars[k] = prior_of_stars[k] * normalizing_fact\n",
    "    return prior_of_stars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_uniqueattributevalues(atrribute_names, training_restaurants_df, unique_dimension_values):\n",
    "    for attribute_name in atrribute_names:\n",
    "        tdf = training_restaurants_df['attributes'].apply(lambda a: extract_value_from_attrs(a, attribute_name))\n",
    "        unique_dimension_values[attribute_name] = tdf.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_working_type(business_of_stars):\n",
    "    working_type_set = set()\n",
    "    for star in business_of_stars:\n",
    "        business_of_star_df = business_of_stars[star]\n",
    "        business_of_star_df['working_type'] = business_of_star_df['working_type'].apply(hours_to_type)\n",
    "        working_type_set |= set(business_of_star_df['working_type'].unique())\n",
    "    return working_type_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_unique_columnvalues(training_restaurants_df, column_names,unique_dimension_values):\n",
    "    for column_name in column_names:\n",
    "        unique_dimension_values[column_name] = training_restaurants_df['city'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DEFAULT_TYPE = 'default'\n",
    "def extract_value_from_attrs(attrs, k):\n",
    "    if k in attrs:\n",
    "        return attrs[k]\n",
    "    else:\n",
    "        return DEFAULT_TYPE\n",
    "    \n",
    "def filter_from_attr_val(attr, k, v):\n",
    "    return k in attr and attr[k] == v\n",
    "\n",
    "def filter_no_attr(attr, k):\n",
    "    return k not in attr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def calculate_frequencies(attributes, dimensions, unique_dimension_values, business_of_stars):\n",
    "    \n",
    "    dimension_freq_map = {}\n",
    "    for dimension in (attributes + dimensions):\n",
    "        dimension_star_map = {}\n",
    "        dimension_freq_map[dimension] = dimension_star_map\n",
    "    \n",
    "    #calculate the frequencies\n",
    "    for star in business_of_stars:\n",
    "\n",
    "        business_of_star_df = business_of_stars[star]\n",
    "        num_business = len(business_of_star_df)\n",
    "\n",
    "        for dimension in dimensions:\n",
    "            dim_star_map = dimension_freq_map[dimension]\n",
    "            dim_freq = {}\n",
    "            dim_of_business = business_of_star_df.groupby(dimension)\n",
    "            num_unique_dimensions = len(unique_dimension_values[dimension])\n",
    "            for grp in dim_of_business.groups:\n",
    "                # we use the add-one or Laplace smoothing\n",
    "                dim_freq[grp] = (len(dim_of_business.get_group(grp)) + 1.0) / (num_business + num_unique_dimensions)\n",
    "            dim_freq[DEFAULT_TYPE] = 1.0 / (num_business + num_unique_dimensions)\n",
    "            dim_star_map[star] = dim_freq\n",
    "\n",
    "        for attribute in attributes:\n",
    "            attr_star_map = dimension_freq_map[attribute]\n",
    "            attribute_freq = {}\n",
    "            attr_set = unique_dimension_values[attribute]\n",
    "            for t in attr_set:\n",
    "                if t != DEFAULT_TYPE:\n",
    "                    num = len(business_of_star_df[business_of_star_df['attributes'].apply(lambda attr: filter_from_attr_val(attr, attribute, t))])\n",
    "                else:\n",
    "                    num = len(business_of_star_df[business_of_star_df['attributes'].apply(lambda attr: filter_no_attr(attr, attribute))])\n",
    "                attribute_freq[t] = (num + 1.0) / (num_business + len(attr_set))\n",
    "            if DEFAULT_TYPE not in  attribute_freq:\n",
    "                attribute_freq[DEFAULT_TYPE] = 1.0 / (num_business + len(attr_set))\n",
    "            attr_star_map[star] = attribute_freq\n",
    "            \n",
    "    return dimension_freq_map\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verification our test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import operator\n",
    "\n",
    "def predict(probs):\n",
    "    sorted_probs = sorted(probs.items(), key=operator.itemgetter(1))\n",
    "    return sorted_probs[-1][0]\n",
    "    \n",
    "def correctness(stars, estimated_stars):\n",
    "    return stars == estimated_stars\n",
    "    \n",
    "def distance(stars, estimated_stars):\n",
    "    return abs(stars - estimated_stars)\n",
    "\n",
    "def calc_probs(row_value, dim_freq_map, selected_columns, prior_of_stars):#hours, city, attrs, sentiment_value, weighted, tip_sentiment, checkin_count):\n",
    "    #print (row_value)\n",
    "    probs_of_stars = {}\n",
    "    \n",
    "    working_type = hours_to_type(row_value['hours'])\n",
    "    #print (working_type)\n",
    "    for star in prior_of_stars:\n",
    "        prob = np.log(prior_of_stars[star])\n",
    "        types_freq_of_stars = dim_freq_map[working_type_column]\n",
    "        #print (types_freq_of_stars)\n",
    "        prob += np.log(types_freq_of_stars[star].get(working_type, types_freq_of_stars[star]['default']))\n",
    "        \n",
    "        for dimension in selected_columns:\n",
    "            dim_freq_star_map = dim_freq_map[dimension]\n",
    "            prob += np.log(dim_freq_star_map[star].get(row_value[dimension], dim_freq_star_map[star]['default']))\n",
    "        \n",
    "        attrs = row_value['attributes']\n",
    "        for attribute in atrribute_names:  \n",
    "            dim_freq_star_map = dim_freq_map[attribute]\n",
    "            attrcol = extract_value_from_attrs(attrs, attribute)\n",
    "            #print (attribute, attrcol)\n",
    "            #print (dim_freq_star_map[star][DEFAULT_TYPE], \"\\n\")\n",
    "            prob += np.log(dim_freq_star_map[star].get(attrcol, dim_freq_star_map[star][DEFAULT_TYPE]))\n",
    "        probs_of_stars[star] = prob\n",
    "    return probs_of_stars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "atrribute_names = ['Accepts Credit Cards','Alcohol','Caters','Noise Level','Price Range','Take-out']\n",
    "column_names = ['review_count','city','review_sentiment_rating','review_star_rating','tip_rating','checkin_rating']\n",
    "working_type_column = 'working_type'\n",
    "\n",
    "def trainNB(training_restaurants_df):\n",
    "    #group by stars \n",
    "    business_of_stars = get_business_bystars(training_restaurants_df,'stars')\n",
    "    #get priors\n",
    "    prior_of_stars = get_priors(business_of_stars,training_restaurants_df )\n",
    "    #get unique values for attributes\n",
    "    unique_dimension_values = {}\n",
    "    get_uniqueattributevalues(atrribute_names, training_restaurants_df, unique_dimension_values)\n",
    "     # unique values for columns\n",
    "    get_unique_columnvalues(training_restaurants_df, column_names,  unique_dimension_values)\n",
    "    unique_dimension_values[working_type_column] = get_working_type(business_of_stars)\n",
    "    \n",
    "    dimension_frequency_map = calculate_frequencies(atrribute_names,column_names+[working_type_column], unique_dimension_values, business_of_stars)\n",
    "    return dimension_frequency_map, prior_of_stars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def testNB(test_restaurants_df, dim_freq_map, selected_columns, prior_of_stars):\n",
    "    result = pd.DataFrame()\n",
    "    result['stars'] = test_restaurants_df['stars']\n",
    "    result['stars_probs'] = test_restaurants_df.apply(lambda r: calc_probs(r, dim_freq_map, selected_columns, prior_of_stars), axis=1)\n",
    "    result['estimated_stars'] = result.apply(lambda r: predict(r['stars_probs']), axis=1)\n",
    "    #write_df_to_json_file(test_restaurants_df[['stars','estimated_weighted_stars','estimated_stars']],\"../out/results.json\")\n",
    "    result['correctness'] = result.apply(lambda r: correctness(r['stars'], r['estimated_stars']), axis=1)\n",
    "    corrects = len(result[result['correctness'] == True])\n",
    "    result['distance'] = result.apply(lambda r: distance(r['stars'], r['estimated_stars']), axis=1)\n",
    "    result['diff'] = result.apply(lambda r: r['stars'] - r['estimated_stars'], axis=1)\n",
    "    result_t = result[result['diff'].apply(lambda x: abs(x) >= 0.5)]\n",
    "    accuracy =  corrects * 1.0 / len(result)\n",
    "    avg_dist = result['distance'].mean()\n",
    "    off_by_morethan_halfstar = len(result_t)\n",
    "    return accuracy,avg_dist,off_by_morethan_halfstar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#80% training data\n",
    "def test_trainsplit(df,fraction = .8):\n",
    "    training_restaurants_df = df.sample(frac=fraction, random_state = 42)\n",
    "    test_restaurants_df = df[~df.isin(training_restaurants_df)].dropna()\n",
    "    return training_restaurants_df, test_restaurants_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculte the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With review -- accuracy,dist,offcount : 0.9402173913043478 0.06023550724637681 132\n",
      "weighted -- accuracy,dist,offcount : 0.7862318840579711 0.24230072463768115 472\n"
     ]
    }
   ],
   "source": [
    "training, test = test_trainsplit(df_business_restaurants)\n",
    "dim_freq_map,prior_of_stars = trainNB(training)\n",
    "selected_columns = ['review_count','city','review_sentiment_rating','review_star_rating','tip_rating','checkin_rating']\n",
    "accuracy,dist,offcount = testNB (test,dim_freq_map, selected_columns,prior_of_stars)\n",
    "print (\"With review -- accuracy,dist,offcount :\",accuracy,dist,offcount)\n",
    "selected_columns = ['review_count','city','review_sentiment_rating','tip_rating','checkin_rating']\n",
    "accuracy,dist,offcount = testNB (test,dim_freq_map, selected_columns,prior_of_stars)\n",
    "print (\"weighted -- accuracy,dist,offcount :\",accuracy,dist,offcount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weighted -- accuracy,dist,offcount : 0.801177536231884 0.22554347826086957 439\n"
     ]
    }
   ],
   "source": [
    "selected_columns = ['city','review_sentiment_rating','tip_rating','checkin_rating']\n",
    "accuracy,dist,offcount = testNB (test,dim_freq_map, selected_columns,prior_of_stars)\n",
    "print (\"weighted -- accuracy,dist,offcount :\",accuracy,dist,offcount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_kfolds(df, folds = 10):\n",
    "    df_test = []\n",
    "    df_train = []\n",
    "    newdf = df\n",
    "    for i in range(0,9):\n",
    "        df_part = newdf.sample(frac= (1/(folds - i)), random_state = 42)\n",
    "        df_test.append(df_part)\n",
    "        df_train.append(df[~df.isin(df_part)].dropna())\n",
    "        newdf = newdf[~newdf.isin(df_part)].dropna()\n",
    "    df_test.append(newdf) \n",
    "    df_train.append(df[~df.isin(newdf)].dropna())\n",
    "    return df_test,df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def k_fold_crossvalidation():\n",
    "    testlist, trainlist = get_kfolds(df_business_restaurants)\n",
    "    accuracy_list = []\n",
    "    dist_list = []\n",
    "    offcount_list = []\n",
    "    for i in range(0, 10):\n",
    "        test = testlist[i]\n",
    "        train = trainlist[i]\n",
    "        dim_freq_map,prior_of_stars = trainNB(train)\n",
    "        selected_columns = ['city','review_sentiment_rating','review_star_rating','tip_rating','checkin_rating']\n",
    "        accuracy,dist,offcount = testNB (test,dim_freq_map, selected_columns,prior_of_stars)\n",
    "        accuracy_list.append(accuracy)\n",
    "        dist_list.append(dist)\n",
    "        offcount_list.append(offcount)\n",
    "    return np.mean(np.asarray(accuracy_list)), np.mean(np.asarray(dist_list)), np.mean(np.asarray(offcount_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.94213054954423237, 0.06040527247688373, 63.899999999999999)\n"
     ]
    }
   ],
   "source": [
    "print (k_fold_crossvalidation())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
